{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (3.2.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.17.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.24.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.34.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn!=0.22.0->lightgbm) (1.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import apiquery\n",
    "import pandas as pd\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "DATA_PATH = '../01.Data'\n",
    "shutil.copy(\"apiquery_pyc.py\", \"apiquery.pyc\")\n",
    "module_path = \"../src\"\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from utils.training import *\n",
    "from utils.encoding import *\n",
    "from utils.utils import *\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import time\n",
    "!pip install lightgbm\n",
    "import lightgbm as lgbm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    df['diff_t']  = df['last_modified_t']-df['created_t']\n",
    "    df.drop(columns = ['last_modified_t','created_t'],inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.9 s, sys: 1.1 s, total: 42 s\n",
      "Wall time: 42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Reading dataframes\n",
    "df_train     = pd.read_csv(os.path.join(DATA_PATH,'food_train.tsv'), index_col='Index', encoding='utf-8', sep='\\t').reset_index(drop = True)\n",
    "df_test      = pd.read_csv(os.path.join(DATA_PATH,'food_X_test.tsv'), index_col='Index', encoding='utf-8', sep='\\t').reset_index()\n",
    "df_test      = df_test.merge(pd.read_csv(os.path.join(DATA_PATH,'target_test.csv')),how = 'left',on = ['Index'])\n",
    "y_submission = pd.read_csv(os.path.join(DATA_PATH,'y_test_submission_example.tsv'), index_col='Index', encoding='utf-8', sep='\\t')\n",
    "\n",
    "\n",
    "cols_extract_date = ['created_datetime','last_modified_datetime']\n",
    "# Preprocessing\n",
    "#2. \n",
    "multiple_extract_time(df_train,cols_extract_date)\n",
    "multiple_extract_time(df_test,cols_extract_date)\n",
    "#3. \n",
    "col = 'main_category'\n",
    "df_train[[f'{col}_lang',f'{col}_cat']] = df_train.apply(lambda x: split_double_dot(x,col),axis = 1,result_type='expand')\n",
    "df_test[[f'{col}_lang',f'{col}_cat']] = df_test.apply(lambda x: split_double_dot(x,col),axis = 1,result_type='expand')\n",
    "\n",
    "col = 'main_category_en'\n",
    "df_train[[f'{col}_lang',f'{col}_cat']] = df_train.apply(lambda x: split_double_dot(x,col),axis = 1,result_type='expand')\n",
    "df_test[[f'{col}_lang',f'{col}_cat']] = df_test.apply(lambda x: split_double_dot(x,col),axis = 1,result_type='expand')\n",
    "\n",
    "\n",
    "# Formating columns\n",
    "format_cols = ['pnns_groups_1','pnns_groups_2','product_name','generic_name','packaging','packaging_tags','brands','brands_tags',\n",
    " 'categories','categories_tags','origins','origins_tags','manufacturing_places','manufacturing_places_tags',\n",
    "'labels','labels_tags','labels_en','emb_codes','emb_codes_tags','cities_tags','purchase_places','stores']\n",
    "for i in format_cols :\n",
    "    df_train[i] = df_train[i].apply(lambda x:func_formatting(x)) \n",
    "    df_test[i] = df_test[i].apply(lambda x:func_formatting(x))\n",
    "\n",
    "# Extracting geo location:\n",
    "df_train[[f'first_packaging_code_geo_x','first_packaging_code_geo_y']] = df_train.apply(lambda x: func_geolocation(x),axis = 1,result_type='expand')\n",
    "df_test[[f'first_packaging_code_geo_x','first_packaging_code_geo_y']]  = df_test.apply(lambda x: func_geolocation(x),axis = 1,result_type='expand')\n",
    "df_train.drop(columns = ['first_packaging_code_geo'],inplace = True)\n",
    "df_test.drop(columns = ['first_packaging_code_geo'],inplace = True)\n",
    "\n",
    "for i in ['countries_en','allergens']:\n",
    "    df_train[i] = df_train[i].apply(lambda x:func_sort_split_comma(x))\n",
    "    df_test[i]  = df_test[i].apply(lambda x:func_sort_split_comma(x))\n",
    "\n",
    "\n",
    "cols_states_en = ['brands','categories','characteristics','expiration date','general_check','general_complete',\n",
    "                 'ingredients','nutrition facts','packaging','packaging-code-','photo_upload','photo_validate',\n",
    "                 'product name','quantity']\n",
    "col = 'states_en'\n",
    "df_train[[f'{col}_{idx}' for idx in cols_states_en]] = df_train.apply(lambda x: func_states(x,col),axis = 1,result_type='expand')\n",
    "df_test[[f'{col}_{idx}' for idx in cols_states_en]] = df_test.apply(lambda x: func_states(x,col),axis = 1,result_type='expand')\n",
    "\n",
    "# Drop some cols\n",
    "cols_drop = ['ingredients_that_may_be_from_palm_oil','ingredients_from_palm_oil','no_nutriments','allergens_en','cities',\n",
    "            'generic_name','categories','categories_en','origins','manufacturing_places','labels',\n",
    "             'emb_codes','emb_codes_tags','cities_tags','purchase_places','stores','countries',\n",
    "             'countries_en','traces','traces_en','additives_en','ingredients_from_palm_oil_tags',\n",
    "             'ingredients_that_may_be_from_palm_oil_tags','ingredients_that_may_be_from_palm_oil_tags',\n",
    "             'states','states_tags','main_category','main_category_en','first_packaging_code_geo_x','first_packaging_code_geo_y']\n",
    "\n",
    "#df_train.drop(columns = cols_drop,inplace = True)\n",
    "#df_test.drop(columns = cols_drop,inplace = True)\n",
    "\n",
    "col = 'additives'\n",
    "df_train[col] = df_train[col].apply(lambda x:process_aditives(x))\n",
    "df_test[col]  = df_test[col].apply(lambda x:process_aditives(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering(df_train)\n",
    "feature_engineering(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num bins: 17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2     9.675775\n",
       "10    9.075940\n",
       "7     7.143137\n",
       "8     7.093151\n",
       "0     6.863802\n",
       "5     6.561924\n",
       "14    5.904262\n",
       "16    5.847414\n",
       "12    5.708237\n",
       "13    5.387737\n",
       "3     5.083899\n",
       "1     5.083899\n",
       "4     4.741836\n",
       "6     4.449759\n",
       "15    3.983220\n",
       "9     3.939115\n",
       "11    3.456894\n",
       "Name: target_bin, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_bins = int(np.floor(1 + np.log2(len(df_train))))\n",
    "#num_bins = 10\n",
    "print(f'Num bins: {num_bins}')\n",
    "df_train['target_bin'] = pd.qcut(df_train['target'],num_bins,labels = False)\n",
    "100*df_train['target_bin'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 5\n",
    "SEED  = 42\n",
    "\n",
    "df_fold = df_train.copy()\n",
    "skf = StratifiedKFold(n_splits = FOLDS,shuffle = True, random_state = SEED)\n",
    "df_fold['fold'] = 0\n",
    "for fold,(train_index, test_index) in enumerate(skf.split(df_fold,df_fold['target_bin'])):\n",
    "    df_fold.loc[test_index,'fold'] = fold\n",
    "    \n",
    "# Save the CSV with folds for training\n",
    "df_fold.to_csv(os.path.join(\"../01.Data\",'fold.csv'),index = False)\n",
    "df_test.to_csv(os.path.join(DATA_PATH,'test_preprocessed.csv'),index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    9.167353\n",
       "1    9.173527\n",
       "2    9.168235\n",
       "3    9.171870\n",
       "4    9.172262\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fold.groupby('fold')['target'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.94 s, sys: 351 ms, total: 3.3 s\n",
      "Wall time: 4.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train = pd.read_csv(os.path.join(\"../01.Data\",'fold.csv'))\n",
    "df_test      = pd.read_csv(os.path.join(DATA_PATH,'test_preprocessed.csv'))\n",
    "y_submission = pd.read_csv(os.path.join(DATA_PATH,'y_test_submission_example.tsv'), index_col='Index', encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['states_en_brands', 'states_en_categories', 'states_en_characteristics', 'states_en_expiration date', 'states_en_general_complete', 'states_en_ingredients', 'pnns_groups_1', 'pnns_groups_2', 'states_en_packaging', 'states_en_packaging-code-', 'states_en_photo_upload', 'states_en_photo_validate', 'states_en_product name', 'states_en_quantity']\n"
     ]
    }
   ],
   "source": [
    "columns_modeling = ['additives_n','ingredients_from_palm_oil_n',\n",
    "                    'ingredients_that_may_be_from_palm_oil_n','target',\n",
    "                    'states_en_brands','states_en_categories','states_en_characteristics','states_en_expiration date',\n",
    "                    'states_en_general_complete','states_en_ingredients','pnns_groups_1','pnns_groups_2',\n",
    "                    'states_en_packaging','states_en_packaging-code-','states_en_photo_upload',\n",
    "                    'states_en_photo_validate','states_en_product name','states_en_quantity','diff_t']\n",
    "columns_label = df_train[columns_modeling].select_dtypes(include=['object']).columns.to_list()\n",
    "print(columns_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: Missing as new category\n",
      "Label Encoding:  label_states_en_brands\n",
      "Label Encoding:  label_states_en_categories\n",
      "Label Encoding:  label_states_en_characteristics\n",
      "Label Encoding:  label_states_en_expiration date\n",
      "Label Encoding:  label_states_en_general_complete\n",
      "Label Encoding:  label_states_en_ingredients\n",
      "Label Encoding:  label_pnns_groups_1\n",
      "Label Encoding:  label_pnns_groups_2\n",
      "Label Encoding:  label_states_en_packaging\n",
      "Label Encoding:  label_states_en_packaging-code-\n",
      "Label Encoding:  label_states_en_photo_upload\n",
      "Label Encoding:  label_states_en_photo_validate\n",
      "Label Encoding:  label_states_en_product name\n",
      "Label Encoding:  label_states_en_quantity\n",
      "Mode: Missing as new category\n",
      "Applying Label Encoding:  label_states_en_brands\n",
      "Applying Label Encoding:  label_states_en_categories\n",
      "Applying Label Encoding:  label_states_en_characteristics\n",
      "Applying Label Encoding:  label_states_en_expiration date\n",
      "Applying Label Encoding:  label_states_en_general_complete\n",
      "Applying Label Encoding:  label_states_en_ingredients\n",
      "Applying Label Encoding:  label_pnns_groups_1\n",
      "Applying Label Encoding:  label_pnns_groups_2\n",
      "Applying Label Encoding:  label_states_en_packaging\n",
      "Applying Label Encoding:  label_states_en_packaging-code-\n",
      "Applying Label Encoding:  label_states_en_photo_upload\n",
      "Applying Label Encoding:  label_states_en_photo_validate\n",
      "Applying Label Encoding:  label_states_en_product name\n",
      "Applying Label Encoding:  label_states_en_quantity\n"
     ]
    }
   ],
   "source": [
    "df_train,dict_le = label_encoding(df_train,label_cols = columns_label, drop_original = True, missing_new_cat = True)\n",
    "df_test = apply_label_encoder(df_test,dict_le,drop_original = True, missing_new_cat = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['diff_t', 'ingredients_from_palm_oil_n', 'target', 'additives_n', 'ingredients_that_may_be_from_palm_oil_n', 'fold', 'label_states_en_brands', 'label_states_en_categories', 'label_states_en_characteristics', 'label_states_en_expiration date', 'label_states_en_general_complete', 'label_states_en_ingredients', 'label_pnns_groups_1', 'label_pnns_groups_2', 'label_states_en_packaging', 'label_states_en_packaging-code-', 'label_states_en_photo_upload', 'label_states_en_photo_validate', 'label_states_en_product name', 'label_states_en_quantity', 'created_datetime_year']\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': {'rmse'},\n",
    "        'num_leaves':10,\n",
    "        'learning_rate': 0.01,\n",
    "        \"min_child_samples\": 150,\n",
    "        \"max_depth\" : 5,\n",
    "        'feature_fraction':  0.7,\n",
    "        \"bagging_freq\": 1,\n",
    "        'bagging_fraction': 0.75,\n",
    "        \"is_unbalance\" : False,\n",
    "        'force_col_wise':True,\n",
    "        'num_threads':18,\n",
    "        #\"scale_pos_weight\":5 -> Generally  is the ratio of number of negative class to the positive class.\n",
    "        'bagging_seed':SEED,\n",
    "        'lambda_l1':1.5,\n",
    "        'lambda_l2':1,\n",
    "        'verbose': 1\n",
    "\n",
    "}\n",
    "cat_columns = [i for i in df_train.columns.to_list() if i.startswith('label_')] +['created_datetime_year']\n",
    "columns_modeling_last = list(set(columns_modeling)-set(columns_label)) + ['fold'] + cat_columns \n",
    "print(columns_modeling_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['diff_t', 'ingredients_from_palm_oil_n', 'additives_n', 'ingredients_that_may_be_from_palm_oil_n', 'label_states_en_brands', 'label_states_en_categories', 'label_states_en_characteristics', 'label_states_en_expiration date', 'label_states_en_general_complete', 'label_states_en_ingredients', 'label_pnns_groups_1', 'label_pnns_groups_2', 'label_states_en_packaging', 'label_states_en_packaging-code-', 'label_states_en_photo_upload', 'label_states_en_photo_validate', 'label_states_en_product name', 'label_states_en_quantity', 'created_datetime_year']\n",
      "Cat index: [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "---------- Training fold Nº 1 ----------\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Total Bins 388\n",
      "[LightGBM] [Info] Number of data points in the train set: 81622, number of used features: 19\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 9.171473\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's rmse: 8.52079\tvalid_1's rmse: 8.52531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's rmse: 8.28488\tvalid_1's rmse: 8.29514\n",
      "[150]\ttraining's rmse: 8.17413\tvalid_1's rmse: 8.18926\n",
      "[200]\ttraining's rmse: 8.11907\tvalid_1's rmse: 8.13754\n",
      "[250]\ttraining's rmse: 8.08934\tvalid_1's rmse: 8.11\n",
      "[300]\ttraining's rmse: 8.07389\tvalid_1's rmse: 8.09632\n",
      "[350]\ttraining's rmse: 8.06369\tvalid_1's rmse: 8.08725\n",
      "[400]\ttraining's rmse: 8.05641\tvalid_1's rmse: 8.08122\n",
      "[450]\ttraining's rmse: 8.05074\tvalid_1's rmse: 8.0765\n",
      "[500]\ttraining's rmse: 8.04579\tvalid_1's rmse: 8.07259\n",
      "[550]\ttraining's rmse: 8.04187\tvalid_1's rmse: 8.06952\n",
      "[600]\ttraining's rmse: 8.03834\tvalid_1's rmse: 8.06682\n",
      "[650]\ttraining's rmse: 8.03505\tvalid_1's rmse: 8.06444\n",
      "[700]\ttraining's rmse: 8.03213\tvalid_1's rmse: 8.06242\n",
      "[750]\ttraining's rmse: 8.0294\tvalid_1's rmse: 8.06055\n",
      "[800]\ttraining's rmse: 8.02709\tvalid_1's rmse: 8.05914\n",
      "[850]\ttraining's rmse: 8.02476\tvalid_1's rmse: 8.05752\n",
      "[900]\ttraining's rmse: 8.02255\tvalid_1's rmse: 8.05601\n",
      "[950]\ttraining's rmse: 8.02059\tvalid_1's rmse: 8.05485\n",
      "[1000]\ttraining's rmse: 8.01879\tvalid_1's rmse: 8.05382\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1050]\ttraining's rmse: 8.01701\tvalid_1's rmse: 8.05281\n",
      "[1100]\ttraining's rmse: 8.01519\tvalid_1's rmse: 8.05172\n",
      "[1150]\ttraining's rmse: 8.01359\tvalid_1's rmse: 8.05075\n",
      "[1200]\ttraining's rmse: 8.01191\tvalid_1's rmse: 8.04985\n",
      "[1250]\ttraining's rmse: 8.01046\tvalid_1's rmse: 8.0489\n",
      "[1300]\ttraining's rmse: 8.009\tvalid_1's rmse: 8.04815\n",
      "[1350]\ttraining's rmse: 8.00767\tvalid_1's rmse: 8.04737\n",
      "[1400]\ttraining's rmse: 8.00625\tvalid_1's rmse: 8.04652\n",
      "[1450]\ttraining's rmse: 8.00493\tvalid_1's rmse: 8.04585\n",
      "[1500]\ttraining's rmse: 8.00369\tvalid_1's rmse: 8.04528\n",
      "[1550]\ttraining's rmse: 8.00259\tvalid_1's rmse: 8.04471\n",
      "[1600]\ttraining's rmse: 8.00147\tvalid_1's rmse: 8.04414\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1650]\ttraining's rmse: 8.00045\tvalid_1's rmse: 8.04374\n",
      "[1700]\ttraining's rmse: 7.99941\tvalid_1's rmse: 8.04331\n",
      "[1750]\ttraining's rmse: 7.99839\tvalid_1's rmse: 8.04281\n",
      "[1800]\ttraining's rmse: 7.99743\tvalid_1's rmse: 8.04247\n",
      "[1850]\ttraining's rmse: 7.99646\tvalid_1's rmse: 8.04208\n",
      "[1900]\ttraining's rmse: 7.99554\tvalid_1's rmse: 8.0416\n",
      "[1950]\ttraining's rmse: 7.99453\tvalid_1's rmse: 8.04124\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2000]\ttraining's rmse: 7.99358\tvalid_1's rmse: 8.04084\n",
      "[2050]\ttraining's rmse: 7.99258\tvalid_1's rmse: 8.04033\n",
      "[2100]\ttraining's rmse: 7.99167\tvalid_1's rmse: 8.03983\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2150]\ttraining's rmse: 7.99077\tvalid_1's rmse: 8.03953\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2200]\ttraining's rmse: 7.98995\tvalid_1's rmse: 8.03936\n",
      "[2250]\ttraining's rmse: 7.98918\tvalid_1's rmse: 8.03901\n",
      "[2300]\ttraining's rmse: 7.98836\tvalid_1's rmse: 8.03876\n",
      "[2350]\ttraining's rmse: 7.98754\tvalid_1's rmse: 8.03832\n",
      "[2400]\ttraining's rmse: 7.98673\tvalid_1's rmse: 8.03806\n",
      "[2450]\ttraining's rmse: 7.98598\tvalid_1's rmse: 8.03793\n",
      "[2500]\ttraining's rmse: 7.98528\tvalid_1's rmse: 8.03786\n",
      "[2550]\ttraining's rmse: 7.98449\tvalid_1's rmse: 8.03752\n",
      "[2600]\ttraining's rmse: 7.98377\tvalid_1's rmse: 8.03733\n",
      "[2650]\ttraining's rmse: 7.98307\tvalid_1's rmse: 8.03703\n",
      "[2700]\ttraining's rmse: 7.98239\tvalid_1's rmse: 8.03689\n",
      "[2750]\ttraining's rmse: 7.98175\tvalid_1's rmse: 8.03664\n",
      "[2800]\ttraining's rmse: 7.98107\tvalid_1's rmse: 8.03642\n",
      "[2850]\ttraining's rmse: 7.98037\tvalid_1's rmse: 8.03625\n",
      "[2900]\ttraining's rmse: 7.97968\tvalid_1's rmse: 8.03602\n",
      "[2950]\ttraining's rmse: 7.97906\tvalid_1's rmse: 8.03585\n",
      "[3000]\ttraining's rmse: 7.9784\tvalid_1's rmse: 8.03569\n",
      "[3050]\ttraining's rmse: 7.97776\tvalid_1's rmse: 8.03551\n",
      "[3100]\ttraining's rmse: 7.97716\tvalid_1's rmse: 8.03543\n",
      "[3150]\ttraining's rmse: 7.97655\tvalid_1's rmse: 8.03526\n",
      "[3200]\ttraining's rmse: 7.97595\tvalid_1's rmse: 8.0351\n",
      "[3250]\ttraining's rmse: 7.97532\tvalid_1's rmse: 8.0349\n",
      "[3300]\ttraining's rmse: 7.97471\tvalid_1's rmse: 8.03485\n",
      "[3350]\ttraining's rmse: 7.97413\tvalid_1's rmse: 8.03479\n",
      "[3400]\ttraining's rmse: 7.97359\tvalid_1's rmse: 8.03464\n",
      "[3450]\ttraining's rmse: 7.97304\tvalid_1's rmse: 8.03454\n",
      "[3500]\ttraining's rmse: 7.97247\tvalid_1's rmse: 8.03427\n",
      "[3550]\ttraining's rmse: 7.97189\tvalid_1's rmse: 8.03405\n",
      "[3600]\ttraining's rmse: 7.97129\tvalid_1's rmse: 8.03406\n",
      "[3650]\ttraining's rmse: 7.97078\tvalid_1's rmse: 8.03394\n",
      "[3700]\ttraining's rmse: 7.97021\tvalid_1's rmse: 8.03381\n",
      "[3750]\ttraining's rmse: 7.96966\tvalid_1's rmse: 8.03364\n",
      "[3800]\ttraining's rmse: 7.96909\tvalid_1's rmse: 8.03342\n",
      "[3850]\ttraining's rmse: 7.9686\tvalid_1's rmse: 8.03338\n",
      "[3900]\ttraining's rmse: 7.96809\tvalid_1's rmse: 8.03332\n",
      "[3950]\ttraining's rmse: 7.96757\tvalid_1's rmse: 8.03329\n",
      "[4000]\ttraining's rmse: 7.96709\tvalid_1's rmse: 8.0333\n",
      "[4050]\ttraining's rmse: 7.96661\tvalid_1's rmse: 8.03323\n",
      "[4100]\ttraining's rmse: 7.96613\tvalid_1's rmse: 8.03319\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4150]\ttraining's rmse: 7.96565\tvalid_1's rmse: 8.0331\n",
      "[4200]\ttraining's rmse: 7.96513\tvalid_1's rmse: 8.03291\n",
      "[4250]\ttraining's rmse: 7.96467\tvalid_1's rmse: 8.03286\n",
      "[4300]\ttraining's rmse: 7.96419\tvalid_1's rmse: 8.03285\n",
      "[4350]\ttraining's rmse: 7.96366\tvalid_1's rmse: 8.03274\n",
      "[4400]\ttraining's rmse: 7.96316\tvalid_1's rmse: 8.03273\n",
      "[4450]\ttraining's rmse: 7.96269\tvalid_1's rmse: 8.03269\n",
      "[4500]\ttraining's rmse: 7.96224\tvalid_1's rmse: 8.0327\n",
      "[4550]\ttraining's rmse: 7.96175\tvalid_1's rmse: 8.0325\n",
      "[4600]\ttraining's rmse: 7.96129\tvalid_1's rmse: 8.03241\n",
      "[4650]\ttraining's rmse: 7.96081\tvalid_1's rmse: 8.03231\n",
      "[4700]\ttraining's rmse: 7.96038\tvalid_1's rmse: 8.03229\n",
      "[4750]\ttraining's rmse: 7.95997\tvalid_1's rmse: 8.03218\n",
      "[4800]\ttraining's rmse: 7.95953\tvalid_1's rmse: 8.03201\n",
      "[4850]\ttraining's rmse: 7.95912\tvalid_1's rmse: 8.03178\n",
      "[4900]\ttraining's rmse: 7.95866\tvalid_1's rmse: 8.0317\n",
      "[4950]\ttraining's rmse: 7.9582\tvalid_1's rmse: 8.0316\n",
      "[5000]\ttraining's rmse: 7.95777\tvalid_1's rmse: 8.0315\n",
      "[5050]\ttraining's rmse: 7.95734\tvalid_1's rmse: 8.03142\n",
      "[5100]\ttraining's rmse: 7.9569\tvalid_1's rmse: 8.03129\n",
      "[5150]\ttraining's rmse: 7.9565\tvalid_1's rmse: 8.03116\n",
      "[5200]\ttraining's rmse: 7.95613\tvalid_1's rmse: 8.03123\n",
      "[5250]\ttraining's rmse: 7.95574\tvalid_1's rmse: 8.03118\n",
      "[5300]\ttraining's rmse: 7.95533\tvalid_1's rmse: 8.03127\n",
      "Early stopping, best iteration is:\n",
      "[5144]\ttraining's rmse: 7.95656\tvalid_1's rmse: 8.03114\n",
      "Train RMSE: 7.956555810224773        Valida RMSE: 8.031137079734053\n",
      "Columns: ['diff_t', 'ingredients_from_palm_oil_n', 'additives_n', 'ingredients_that_may_be_from_palm_oil_n', 'label_states_en_brands', 'label_states_en_categories', 'label_states_en_characteristics', 'label_states_en_expiration date', 'label_states_en_general_complete', 'label_states_en_ingredients', 'label_pnns_groups_1', 'label_pnns_groups_2', 'label_states_en_packaging', 'label_states_en_packaging-code-', 'label_states_en_photo_upload', 'label_states_en_photo_validate', 'label_states_en_product name', 'label_states_en_quantity', 'created_datetime_year']\n",
      "Cat index: [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "---------- Training fold Nº 2 ----------\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Total Bins 388\n",
      "[LightGBM] [Info] Number of data points in the train set: 81622, number of used features: 19\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 9.169930\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's rmse: 8.51885\tvalid_1's rmse: 8.52845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's rmse: 8.28431\tvalid_1's rmse: 8.29613\n",
      "[150]\ttraining's rmse: 8.1748\tvalid_1's rmse: 8.18859\n",
      "[200]\ttraining's rmse: 8.12016\tvalid_1's rmse: 8.13561\n",
      "[250]\ttraining's rmse: 8.09103\tvalid_1's rmse: 8.10779\n",
      "[300]\ttraining's rmse: 8.07535\tvalid_1's rmse: 8.09319\n",
      "[350]\ttraining's rmse: 8.06535\tvalid_1's rmse: 8.08416\n",
      "[400]\ttraining's rmse: 8.05826\tvalid_1's rmse: 8.07775\n",
      "[450]\ttraining's rmse: 8.05251\tvalid_1's rmse: 8.07268\n",
      "[500]\ttraining's rmse: 8.04799\tvalid_1's rmse: 8.06893\n",
      "[550]\ttraining's rmse: 8.04372\tvalid_1's rmse: 8.06541\n",
      "[600]\ttraining's rmse: 8.04021\tvalid_1's rmse: 8.06222\n",
      "[650]\ttraining's rmse: 8.03712\tvalid_1's rmse: 8.05962\n",
      "[700]\ttraining's rmse: 8.03444\tvalid_1's rmse: 8.05758\n",
      "[750]\ttraining's rmse: 8.03167\tvalid_1's rmse: 8.05554\n",
      "[800]\ttraining's rmse: 8.02948\tvalid_1's rmse: 8.05372\n",
      "[850]\ttraining's rmse: 8.0271\tvalid_1's rmse: 8.0518\n",
      "[900]\ttraining's rmse: 8.02497\tvalid_1's rmse: 8.05035\n",
      "[950]\ttraining's rmse: 8.0231\tvalid_1's rmse: 8.04889\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1000]\ttraining's rmse: 8.02119\tvalid_1's rmse: 8.04737\n",
      "[1050]\ttraining's rmse: 8.01942\tvalid_1's rmse: 8.04607\n",
      "[1100]\ttraining's rmse: 8.01764\tvalid_1's rmse: 8.04475\n",
      "[1150]\ttraining's rmse: 8.0161\tvalid_1's rmse: 8.04356\n",
      "[1200]\ttraining's rmse: 8.01459\tvalid_1's rmse: 8.04258\n",
      "[1250]\ttraining's rmse: 8.01321\tvalid_1's rmse: 8.04183\n",
      "[1300]\ttraining's rmse: 8.01174\tvalid_1's rmse: 8.04096\n",
      "[1350]\ttraining's rmse: 8.01041\tvalid_1's rmse: 8.04028\n",
      "[1400]\ttraining's rmse: 8.00914\tvalid_1's rmse: 8.0394\n",
      "[1450]\ttraining's rmse: 8.00794\tvalid_1's rmse: 8.03869\n",
      "[1500]\ttraining's rmse: 8.00673\tvalid_1's rmse: 8.03792\n",
      "[1550]\ttraining's rmse: 8.00563\tvalid_1's rmse: 8.03743\n",
      "[1600]\ttraining's rmse: 8.00455\tvalid_1's rmse: 8.03671\n",
      "[1650]\ttraining's rmse: 8.00338\tvalid_1's rmse: 8.03604\n",
      "[1700]\ttraining's rmse: 8.0024\tvalid_1's rmse: 8.03554\n",
      "[1750]\ttraining's rmse: 8.00141\tvalid_1's rmse: 8.035\n",
      "[1800]\ttraining's rmse: 8.00042\tvalid_1's rmse: 8.03457\n",
      "[1850]\ttraining's rmse: 7.99938\tvalid_1's rmse: 8.034\n",
      "[1900]\ttraining's rmse: 7.99837\tvalid_1's rmse: 8.03345\n",
      "[1950]\ttraining's rmse: 7.99743\tvalid_1's rmse: 8.03301\n",
      "[2000]\ttraining's rmse: 7.99658\tvalid_1's rmse: 8.03269\n",
      "[2050]\ttraining's rmse: 7.99575\tvalid_1's rmse: 8.0324\n",
      "[2100]\ttraining's rmse: 7.99495\tvalid_1's rmse: 8.03213\n",
      "[2150]\ttraining's rmse: 7.99398\tvalid_1's rmse: 8.03181\n",
      "[2200]\ttraining's rmse: 7.99317\tvalid_1's rmse: 8.03158\n",
      "[2250]\ttraining's rmse: 7.99231\tvalid_1's rmse: 8.03121\n",
      "[2300]\ttraining's rmse: 7.99156\tvalid_1's rmse: 8.0309\n",
      "[2350]\ttraining's rmse: 7.99079\tvalid_1's rmse: 8.03061\n",
      "[2400]\ttraining's rmse: 7.99004\tvalid_1's rmse: 8.03037\n",
      "[2450]\ttraining's rmse: 7.98929\tvalid_1's rmse: 8.03011\n",
      "[2500]\ttraining's rmse: 7.98856\tvalid_1's rmse: 8.02986\n",
      "[2550]\ttraining's rmse: 7.98776\tvalid_1's rmse: 8.02944\n",
      "[2600]\ttraining's rmse: 7.98701\tvalid_1's rmse: 8.02922\n",
      "[2650]\ttraining's rmse: 7.98625\tvalid_1's rmse: 8.029\n",
      "[2700]\ttraining's rmse: 7.98563\tvalid_1's rmse: 8.02878\n",
      "[2750]\ttraining's rmse: 7.98496\tvalid_1's rmse: 8.0285\n",
      "[2800]\ttraining's rmse: 7.98429\tvalid_1's rmse: 8.02819\n",
      "[2850]\ttraining's rmse: 7.98364\tvalid_1's rmse: 8.02811\n",
      "[2900]\ttraining's rmse: 7.98299\tvalid_1's rmse: 8.02789\n",
      "[2950]\ttraining's rmse: 7.98231\tvalid_1's rmse: 8.02768\n",
      "[3000]\ttraining's rmse: 7.98164\tvalid_1's rmse: 8.02752\n",
      "[3050]\ttraining's rmse: 7.98096\tvalid_1's rmse: 8.02734\n",
      "[3100]\ttraining's rmse: 7.98031\tvalid_1's rmse: 8.02734\n",
      "[3150]\ttraining's rmse: 7.9797\tvalid_1's rmse: 8.02714\n",
      "[3200]\ttraining's rmse: 7.97911\tvalid_1's rmse: 8.02689\n",
      "[3250]\ttraining's rmse: 7.97854\tvalid_1's rmse: 8.02679\n",
      "[3300]\ttraining's rmse: 7.97792\tvalid_1's rmse: 8.02667\n",
      "[3350]\ttraining's rmse: 7.97737\tvalid_1's rmse: 8.0267\n",
      "[3400]\ttraining's rmse: 7.97679\tvalid_1's rmse: 8.02666\n",
      "[3450]\ttraining's rmse: 7.97623\tvalid_1's rmse: 8.02644\n",
      "[3500]\ttraining's rmse: 7.97569\tvalid_1's rmse: 8.02638\n",
      "[3550]\ttraining's rmse: 7.97517\tvalid_1's rmse: 8.02626\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3600]\ttraining's rmse: 7.97459\tvalid_1's rmse: 8.02611\n",
      "[3650]\ttraining's rmse: 7.97407\tvalid_1's rmse: 8.02598\n",
      "[3700]\ttraining's rmse: 7.97353\tvalid_1's rmse: 8.02593\n",
      "[3750]\ttraining's rmse: 7.97298\tvalid_1's rmse: 8.02585\n",
      "[3800]\ttraining's rmse: 7.97247\tvalid_1's rmse: 8.02566\n",
      "[3850]\ttraining's rmse: 7.97198\tvalid_1's rmse: 8.02559\n",
      "[3900]\ttraining's rmse: 7.97143\tvalid_1's rmse: 8.02554\n",
      "[3950]\ttraining's rmse: 7.9709\tvalid_1's rmse: 8.02545\n",
      "[4000]\ttraining's rmse: 7.97039\tvalid_1's rmse: 8.0253\n",
      "[4050]\ttraining's rmse: 7.96991\tvalid_1's rmse: 8.02529\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4100]\ttraining's rmse: 7.96937\tvalid_1's rmse: 8.02534\n",
      "[4150]\ttraining's rmse: 7.96888\tvalid_1's rmse: 8.0252\n",
      "[4200]\ttraining's rmse: 7.96839\tvalid_1's rmse: 8.02504\n",
      "[4250]\ttraining's rmse: 7.9679\tvalid_1's rmse: 8.02484\n",
      "[4300]\ttraining's rmse: 7.96742\tvalid_1's rmse: 8.02488\n",
      "[4350]\ttraining's rmse: 7.9669\tvalid_1's rmse: 8.02476\n",
      "[4400]\ttraining's rmse: 7.96643\tvalid_1's rmse: 8.02469\n",
      "[4450]\ttraining's rmse: 7.96599\tvalid_1's rmse: 8.02468\n",
      "[4500]\ttraining's rmse: 7.96552\tvalid_1's rmse: 8.02459\n",
      "[4550]\ttraining's rmse: 7.96508\tvalid_1's rmse: 8.02461\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4600]\ttraining's rmse: 7.96463\tvalid_1's rmse: 8.02459\n",
      "[4650]\ttraining's rmse: 7.96422\tvalid_1's rmse: 8.02461\n",
      "[4700]\ttraining's rmse: 7.96376\tvalid_1's rmse: 8.02454\n",
      "[4750]\ttraining's rmse: 7.9633\tvalid_1's rmse: 8.02447\n",
      "[4800]\ttraining's rmse: 7.96285\tvalid_1's rmse: 8.02448\n",
      "[4850]\ttraining's rmse: 7.96243\tvalid_1's rmse: 8.02444\n",
      "[4900]\ttraining's rmse: 7.96202\tvalid_1's rmse: 8.02439\n",
      "[4950]\ttraining's rmse: 7.96161\tvalid_1's rmse: 8.0244\n",
      "[5000]\ttraining's rmse: 7.96117\tvalid_1's rmse: 8.0243\n",
      "[5050]\ttraining's rmse: 7.96074\tvalid_1's rmse: 8.02426\n",
      "[5100]\ttraining's rmse: 7.96031\tvalid_1's rmse: 8.02425\n",
      "[5150]\ttraining's rmse: 7.9599\tvalid_1's rmse: 8.02426\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5200]\ttraining's rmse: 7.95952\tvalid_1's rmse: 8.02421\n",
      "[5250]\ttraining's rmse: 7.95909\tvalid_1's rmse: 8.02424\n",
      "[5300]\ttraining's rmse: 7.95871\tvalid_1's rmse: 8.02425\n",
      "[5350]\ttraining's rmse: 7.95836\tvalid_1's rmse: 8.02424\n",
      "[5400]\ttraining's rmse: 7.95795\tvalid_1's rmse: 8.02419\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5450]\ttraining's rmse: 7.95753\tvalid_1's rmse: 8.0243\n",
      "[5500]\ttraining's rmse: 7.95715\tvalid_1's rmse: 8.02426\n",
      "[5550]\ttraining's rmse: 7.95679\tvalid_1's rmse: 8.02416\n",
      "Early stopping, best iteration is:\n",
      "[5380]\ttraining's rmse: 7.95812\tvalid_1's rmse: 8.02416\n",
      "Train RMSE: 7.95811549325521        Valida RMSE: 8.024160385089697\n",
      "Columns: ['diff_t', 'ingredients_from_palm_oil_n', 'additives_n', 'ingredients_that_may_be_from_palm_oil_n', 'label_states_en_brands', 'label_states_en_categories', 'label_states_en_characteristics', 'label_states_en_expiration date', 'label_states_en_general_complete', 'label_states_en_ingredients', 'label_pnns_groups_1', 'label_pnns_groups_2', 'label_states_en_packaging', 'label_states_en_packaging-code-', 'label_states_en_photo_upload', 'label_states_en_photo_validate', 'label_states_en_product name', 'label_states_en_quantity', 'created_datetime_year']\n",
      "Cat index: [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "---------- Training fold Nº 3 ----------\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Total Bins 388\n",
      "[LightGBM] [Info] Number of data points in the train set: 81622, number of used features: 19\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 9.171253\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's rmse: 8.52402\tvalid_1's rmse: 8.51473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's rmse: 8.28982\tvalid_1's rmse: 8.28107\n",
      "[150]\ttraining's rmse: 8.17996\tvalid_1's rmse: 8.17104\n",
      "[200]\ttraining's rmse: 8.1249\tvalid_1's rmse: 8.11555\n",
      "[250]\ttraining's rmse: 8.09558\tvalid_1's rmse: 8.08636\n",
      "[300]\ttraining's rmse: 8.07985\tvalid_1's rmse: 8.07107\n",
      "[350]\ttraining's rmse: 8.06975\tvalid_1's rmse: 8.0617\n",
      "[400]\ttraining's rmse: 8.06247\tvalid_1's rmse: 8.05496\n",
      "[450]\ttraining's rmse: 8.05644\tvalid_1's rmse: 8.04981\n",
      "[500]\ttraining's rmse: 8.05202\tvalid_1's rmse: 8.04631\n",
      "[550]\ttraining's rmse: 8.04779\tvalid_1's rmse: 8.04292\n",
      "[600]\ttraining's rmse: 8.04431\tvalid_1's rmse: 8.0405\n",
      "[650]\ttraining's rmse: 8.04128\tvalid_1's rmse: 8.03825\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[700]\ttraining's rmse: 8.03846\tvalid_1's rmse: 8.03627\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[750]\ttraining's rmse: 8.03558\tvalid_1's rmse: 8.03433\n",
      "[800]\ttraining's rmse: 8.03317\tvalid_1's rmse: 8.03269\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[850]\ttraining's rmse: 8.03078\tvalid_1's rmse: 8.03119\n",
      "[900]\ttraining's rmse: 8.02869\tvalid_1's rmse: 8.03008\n",
      "[950]\ttraining's rmse: 8.02659\tvalid_1's rmse: 8.02871\n",
      "[1000]\ttraining's rmse: 8.02483\tvalid_1's rmse: 8.02785\n",
      "[1050]\ttraining's rmse: 8.02302\tvalid_1's rmse: 8.02696\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1100]\ttraining's rmse: 8.02143\tvalid_1's rmse: 8.02624\n",
      "[1150]\ttraining's rmse: 8.01968\tvalid_1's rmse: 8.02547\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1200]\ttraining's rmse: 8.01815\tvalid_1's rmse: 8.02476\n",
      "[1250]\ttraining's rmse: 8.0167\tvalid_1's rmse: 8.02411\n",
      "[1300]\ttraining's rmse: 8.01528\tvalid_1's rmse: 8.02343\n",
      "[1350]\ttraining's rmse: 8.01393\tvalid_1's rmse: 8.02281\n",
      "[1400]\ttraining's rmse: 8.01244\tvalid_1's rmse: 8.02227\n",
      "[1450]\ttraining's rmse: 8.01127\tvalid_1's rmse: 8.02176\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1500]\ttraining's rmse: 8.01001\tvalid_1's rmse: 8.02129\n",
      "[1550]\ttraining's rmse: 8.00886\tvalid_1's rmse: 8.02096\n",
      "[1600]\ttraining's rmse: 8.00769\tvalid_1's rmse: 8.02062\n",
      "[1650]\ttraining's rmse: 8.0066\tvalid_1's rmse: 8.02022\n",
      "[1700]\ttraining's rmse: 8.00565\tvalid_1's rmse: 8.01995\n",
      "[1750]\ttraining's rmse: 8.00461\tvalid_1's rmse: 8.01966\n",
      "[1800]\ttraining's rmse: 8.00361\tvalid_1's rmse: 8.01933\n",
      "[1850]\ttraining's rmse: 8.00265\tvalid_1's rmse: 8.01906\n",
      "[1900]\ttraining's rmse: 8.00162\tvalid_1's rmse: 8.01875\n",
      "[1950]\ttraining's rmse: 8.00065\tvalid_1's rmse: 8.01856\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2000]\ttraining's rmse: 7.99976\tvalid_1's rmse: 8.01817\n",
      "[2050]\ttraining's rmse: 7.99888\tvalid_1's rmse: 8.01785\n",
      "[2100]\ttraining's rmse: 7.99799\tvalid_1's rmse: 8.01743\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2150]\ttraining's rmse: 7.99714\tvalid_1's rmse: 8.01731\n",
      "[2200]\ttraining's rmse: 7.99627\tvalid_1's rmse: 8.01723\n",
      "[2250]\ttraining's rmse: 7.99544\tvalid_1's rmse: 8.01708\n",
      "[2300]\ttraining's rmse: 7.99464\tvalid_1's rmse: 8.01683\n",
      "[2350]\ttraining's rmse: 7.99389\tvalid_1's rmse: 8.01664\n",
      "[2400]\ttraining's rmse: 7.99309\tvalid_1's rmse: 8.01641\n",
      "[2450]\ttraining's rmse: 7.99234\tvalid_1's rmse: 8.01619\n",
      "[2500]\ttraining's rmse: 7.99159\tvalid_1's rmse: 8.01605\n",
      "[2550]\ttraining's rmse: 7.99084\tvalid_1's rmse: 8.01597\n",
      "[2600]\ttraining's rmse: 7.99007\tvalid_1's rmse: 8.01588\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2650]\ttraining's rmse: 7.98931\tvalid_1's rmse: 8.01574\n",
      "[2700]\ttraining's rmse: 7.98855\tvalid_1's rmse: 8.0155\n",
      "[2750]\ttraining's rmse: 7.98781\tvalid_1's rmse: 8.01532\n",
      "[2800]\ttraining's rmse: 7.98712\tvalid_1's rmse: 8.0153\n",
      "[2850]\ttraining's rmse: 7.98644\tvalid_1's rmse: 8.01525\n",
      "[2900]\ttraining's rmse: 7.98578\tvalid_1's rmse: 8.0152\n",
      "[2950]\ttraining's rmse: 7.98514\tvalid_1's rmse: 8.01518\n",
      "[3000]\ttraining's rmse: 7.98454\tvalid_1's rmse: 8.01526\n",
      "[3050]\ttraining's rmse: 7.98388\tvalid_1's rmse: 8.0152\n",
      "[3100]\ttraining's rmse: 7.98322\tvalid_1's rmse: 8.01508\n",
      "[3150]\ttraining's rmse: 7.98258\tvalid_1's rmse: 8.015\n",
      "[3200]\ttraining's rmse: 7.98199\tvalid_1's rmse: 8.01488\n",
      "[3250]\ttraining's rmse: 7.98144\tvalid_1's rmse: 8.01482\n",
      "[3300]\ttraining's rmse: 7.98089\tvalid_1's rmse: 8.01479\n",
      "[3350]\ttraining's rmse: 7.9803\tvalid_1's rmse: 8.01483\n",
      "[3400]\ttraining's rmse: 7.97973\tvalid_1's rmse: 8.01483\n",
      "[3450]\ttraining's rmse: 7.97909\tvalid_1's rmse: 8.01478\n",
      "[3500]\ttraining's rmse: 7.97848\tvalid_1's rmse: 8.0147\n",
      "[3550]\ttraining's rmse: 7.97791\tvalid_1's rmse: 8.01457\n",
      "[3600]\ttraining's rmse: 7.97742\tvalid_1's rmse: 8.01458\n",
      "[3650]\ttraining's rmse: 7.97683\tvalid_1's rmse: 8.01454\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3700]\ttraining's rmse: 7.9763\tvalid_1's rmse: 8.01446\n",
      "[3750]\ttraining's rmse: 7.97576\tvalid_1's rmse: 8.0144\n",
      "[3800]\ttraining's rmse: 7.97522\tvalid_1's rmse: 8.01428\n",
      "[3850]\ttraining's rmse: 7.9747\tvalid_1's rmse: 8.0143\n",
      "[3900]\ttraining's rmse: 7.97416\tvalid_1's rmse: 8.01428\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3950]\ttraining's rmse: 7.97367\tvalid_1's rmse: 8.01418\n",
      "[4000]\ttraining's rmse: 7.97316\tvalid_1's rmse: 8.01414\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4050]\ttraining's rmse: 7.97266\tvalid_1's rmse: 8.01403\n",
      "[4100]\ttraining's rmse: 7.97217\tvalid_1's rmse: 8.01398\n",
      "[4150]\ttraining's rmse: 7.97166\tvalid_1's rmse: 8.01406\n",
      "[4200]\ttraining's rmse: 7.97112\tvalid_1's rmse: 8.01411\n",
      "[4250]\ttraining's rmse: 7.97062\tvalid_1's rmse: 8.01408\n",
      "[4300]\ttraining's rmse: 7.97017\tvalid_1's rmse: 8.01404\n",
      "Early stopping, best iteration is:\n",
      "[4105]\ttraining's rmse: 7.97211\tvalid_1's rmse: 8.01398\n",
      "Train RMSE: 7.972107137132723        Valida RMSE: 8.013977668638583\n",
      "Columns: ['diff_t', 'ingredients_from_palm_oil_n', 'additives_n', 'ingredients_that_may_be_from_palm_oil_n', 'label_states_en_brands', 'label_states_en_categories', 'label_states_en_characteristics', 'label_states_en_expiration date', 'label_states_en_general_complete', 'label_states_en_ingredients', 'label_pnns_groups_1', 'label_pnns_groups_2', 'label_states_en_packaging', 'label_states_en_packaging-code-', 'label_states_en_photo_upload', 'label_states_en_photo_validate', 'label_states_en_product name', 'label_states_en_quantity', 'created_datetime_year']\n",
      "Cat index: [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "---------- Training fold Nº 4 ----------\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Total Bins 388\n",
      "[LightGBM] [Info] Number of data points in the train set: 81623, number of used features: 19\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 9.170344\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's rmse: 8.52045\tvalid_1's rmse: 8.5264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's rmse: 8.28559\tvalid_1's rmse: 8.29583\n",
      "[150]\ttraining's rmse: 8.17517\tvalid_1's rmse: 8.18817\n",
      "[200]\ttraining's rmse: 8.11976\tvalid_1's rmse: 8.13463\n",
      "[250]\ttraining's rmse: 8.09017\tvalid_1's rmse: 8.10645\n",
      "[300]\ttraining's rmse: 8.07446\tvalid_1's rmse: 8.09246\n",
      "[350]\ttraining's rmse: 8.06422\tvalid_1's rmse: 8.08421\n",
      "[400]\ttraining's rmse: 8.05656\tvalid_1's rmse: 8.07853\n",
      "[450]\ttraining's rmse: 8.05046\tvalid_1's rmse: 8.07444\n",
      "[500]\ttraining's rmse: 8.04564\tvalid_1's rmse: 8.07148\n",
      "[550]\ttraining's rmse: 8.04132\tvalid_1's rmse: 8.06875\n",
      "[600]\ttraining's rmse: 8.03775\tvalid_1's rmse: 8.06668\n",
      "[650]\ttraining's rmse: 8.03448\tvalid_1's rmse: 8.06491\n",
      "[700]\ttraining's rmse: 8.03157\tvalid_1's rmse: 8.06345\n",
      "[750]\ttraining's rmse: 8.02874\tvalid_1's rmse: 8.06178\n",
      "[800]\ttraining's rmse: 8.02618\tvalid_1's rmse: 8.06038\n",
      "[850]\ttraining's rmse: 8.02377\tvalid_1's rmse: 8.05936\n",
      "[900]\ttraining's rmse: 8.02149\tvalid_1's rmse: 8.05819\n",
      "[950]\ttraining's rmse: 8.01941\tvalid_1's rmse: 8.05726\n",
      "[1000]\ttraining's rmse: 8.01753\tvalid_1's rmse: 8.05639\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1050]\ttraining's rmse: 8.01575\tvalid_1's rmse: 8.05572\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1100]\ttraining's rmse: 8.014\tvalid_1's rmse: 8.05502\n",
      "[1150]\ttraining's rmse: 8.01237\tvalid_1's rmse: 8.05432\n",
      "[1200]\ttraining's rmse: 8.0107\tvalid_1's rmse: 8.05379\n",
      "[1250]\ttraining's rmse: 8.00913\tvalid_1's rmse: 8.05333\n",
      "[1300]\ttraining's rmse: 8.00759\tvalid_1's rmse: 8.05265\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1350]\ttraining's rmse: 8.00617\tvalid_1's rmse: 8.05202\n",
      "[1400]\ttraining's rmse: 8.00481\tvalid_1's rmse: 8.05164\n",
      "[1450]\ttraining's rmse: 8.00352\tvalid_1's rmse: 8.05125\n",
      "[1500]\ttraining's rmse: 8.00224\tvalid_1's rmse: 8.05085\n",
      "[1550]\ttraining's rmse: 8.00105\tvalid_1's rmse: 8.05055\n",
      "[1600]\ttraining's rmse: 7.99991\tvalid_1's rmse: 8.0502\n",
      "[1650]\ttraining's rmse: 7.9988\tvalid_1's rmse: 8.04984\n",
      "[1700]\ttraining's rmse: 7.99779\tvalid_1's rmse: 8.04965\n",
      "[1750]\ttraining's rmse: 7.9967\tvalid_1's rmse: 8.0493\n",
      "[1800]\ttraining's rmse: 7.99577\tvalid_1's rmse: 8.0491\n",
      "[1850]\ttraining's rmse: 7.99479\tvalid_1's rmse: 8.04889\n",
      "[1900]\ttraining's rmse: 7.99376\tvalid_1's rmse: 8.04858\n",
      "[1950]\ttraining's rmse: 7.99284\tvalid_1's rmse: 8.04829\n",
      "[2000]\ttraining's rmse: 7.99196\tvalid_1's rmse: 8.04811\n",
      "[2050]\ttraining's rmse: 7.99107\tvalid_1's rmse: 8.0479\n",
      "[2100]\ttraining's rmse: 7.99021\tvalid_1's rmse: 8.04765\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2150]\ttraining's rmse: 7.98936\tvalid_1's rmse: 8.04737\n",
      "[2200]\ttraining's rmse: 7.98848\tvalid_1's rmse: 8.04717\n",
      "[2250]\ttraining's rmse: 7.98767\tvalid_1's rmse: 8.04685\n",
      "[2300]\ttraining's rmse: 7.98681\tvalid_1's rmse: 8.04658\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2350]\ttraining's rmse: 7.98595\tvalid_1's rmse: 8.04646\n",
      "[2400]\ttraining's rmse: 7.9852\tvalid_1's rmse: 8.04629\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2450]\ttraining's rmse: 7.98449\tvalid_1's rmse: 8.04608\n",
      "[2500]\ttraining's rmse: 7.98376\tvalid_1's rmse: 8.04595\n",
      "[2550]\ttraining's rmse: 7.98299\tvalid_1's rmse: 8.04571\n",
      "[2600]\ttraining's rmse: 7.98226\tvalid_1's rmse: 8.04562\n",
      "[2650]\ttraining's rmse: 7.98158\tvalid_1's rmse: 8.04542\n",
      "[2700]\ttraining's rmse: 7.98088\tvalid_1's rmse: 8.04532\n",
      "[2750]\ttraining's rmse: 7.98019\tvalid_1's rmse: 8.04531\n",
      "[2800]\ttraining's rmse: 7.97947\tvalid_1's rmse: 8.04514\n",
      "[2850]\ttraining's rmse: 7.97885\tvalid_1's rmse: 8.04515\n",
      "[2900]\ttraining's rmse: 7.97823\tvalid_1's rmse: 8.04506\n",
      "[2950]\ttraining's rmse: 7.97756\tvalid_1's rmse: 8.04497\n",
      "[3000]\ttraining's rmse: 7.97694\tvalid_1's rmse: 8.04501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3050]\ttraining's rmse: 7.97631\tvalid_1's rmse: 8.04497\n",
      "[3100]\ttraining's rmse: 7.9757\tvalid_1's rmse: 8.04484\n",
      "[3150]\ttraining's rmse: 7.9751\tvalid_1's rmse: 8.04467\n",
      "[3200]\ttraining's rmse: 7.97448\tvalid_1's rmse: 8.04458\n",
      "[3250]\ttraining's rmse: 7.97388\tvalid_1's rmse: 8.04445\n",
      "[3300]\ttraining's rmse: 7.97332\tvalid_1's rmse: 8.04449\n",
      "[3350]\ttraining's rmse: 7.97272\tvalid_1's rmse: 8.0443\n",
      "[3400]\ttraining's rmse: 7.97211\tvalid_1's rmse: 8.04419\n",
      "[3450]\ttraining's rmse: 7.97152\tvalid_1's rmse: 8.04417\n",
      "[3500]\ttraining's rmse: 7.97097\tvalid_1's rmse: 8.0441\n",
      "[3550]\ttraining's rmse: 7.97042\tvalid_1's rmse: 8.04399\n",
      "[3600]\ttraining's rmse: 7.96986\tvalid_1's rmse: 8.04389\n",
      "[3650]\ttraining's rmse: 7.96933\tvalid_1's rmse: 8.04379\n",
      "[3700]\ttraining's rmse: 7.96881\tvalid_1's rmse: 8.04374\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3750]\ttraining's rmse: 7.96827\tvalid_1's rmse: 8.04365\n",
      "[3800]\ttraining's rmse: 7.96769\tvalid_1's rmse: 8.04366\n",
      "[3850]\ttraining's rmse: 7.96722\tvalid_1's rmse: 8.04375\n",
      "[3900]\ttraining's rmse: 7.9667\tvalid_1's rmse: 8.04367\n",
      "[3950]\ttraining's rmse: 7.96619\tvalid_1's rmse: 8.0436\n",
      "[4000]\ttraining's rmse: 7.96572\tvalid_1's rmse: 8.04354\n",
      "[4050]\ttraining's rmse: 7.96523\tvalid_1's rmse: 8.04342\n",
      "[4100]\ttraining's rmse: 7.96475\tvalid_1's rmse: 8.0435\n",
      "[4150]\ttraining's rmse: 7.9642\tvalid_1's rmse: 8.04352\n",
      "[4200]\ttraining's rmse: 7.96369\tvalid_1's rmse: 8.04352\n",
      "Early stopping, best iteration is:\n",
      "[4045]\ttraining's rmse: 7.96528\tvalid_1's rmse: 8.0434\n",
      "Train RMSE: 7.965280842655302        Valida RMSE: 8.043403157739167\n",
      "Columns: ['diff_t', 'ingredients_from_palm_oil_n', 'additives_n', 'ingredients_that_may_be_from_palm_oil_n', 'label_states_en_brands', 'label_states_en_categories', 'label_states_en_characteristics', 'label_states_en_expiration date', 'label_states_en_general_complete', 'label_states_en_ingredients', 'label_pnns_groups_1', 'label_pnns_groups_2', 'label_states_en_packaging', 'label_states_en_packaging-code-', 'label_states_en_photo_upload', 'label_states_en_photo_validate', 'label_states_en_product name', 'label_states_en_quantity', 'created_datetime_year']\n",
      "Cat index: [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "---------- Training fold Nº 5 ----------\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Total Bins 388\n",
      "[LightGBM] [Info] Number of data points in the train set: 81623, number of used features: 19\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 9.170246\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's rmse: 8.52342\tvalid_1's rmse: 8.52009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's rmse: 8.2904\tvalid_1's rmse: 8.28051\n",
      "[150]\ttraining's rmse: 8.18092\tvalid_1's rmse: 8.16742\n",
      "[200]\ttraining's rmse: 8.12641\tvalid_1's rmse: 8.11069\n",
      "[250]\ttraining's rmse: 8.09759\tvalid_1's rmse: 8.08071\n",
      "[300]\ttraining's rmse: 8.08227\tvalid_1's rmse: 8.06458\n",
      "[350]\ttraining's rmse: 8.07246\tvalid_1's rmse: 8.05445\n",
      "[400]\ttraining's rmse: 8.06513\tvalid_1's rmse: 8.04707\n",
      "[450]\ttraining's rmse: 8.05922\tvalid_1's rmse: 8.04131\n",
      "[500]\ttraining's rmse: 8.05469\tvalid_1's rmse: 8.03689\n",
      "[550]\ttraining's rmse: 8.0507\tvalid_1's rmse: 8.03318\n",
      "[600]\ttraining's rmse: 8.04732\tvalid_1's rmse: 8.0303\n",
      "[650]\ttraining's rmse: 8.04415\tvalid_1's rmse: 8.02774\n",
      "[700]\ttraining's rmse: 8.04132\tvalid_1's rmse: 8.02549\n",
      "[750]\ttraining's rmse: 8.03862\tvalid_1's rmse: 8.02333\n",
      "[800]\ttraining's rmse: 8.03619\tvalid_1's rmse: 8.02146\n",
      "[850]\ttraining's rmse: 8.03384\tvalid_1's rmse: 8.01976\n",
      "[900]\ttraining's rmse: 8.0318\tvalid_1's rmse: 8.01839\n",
      "[950]\ttraining's rmse: 8.02971\tvalid_1's rmse: 8.01701\n",
      "[1000]\ttraining's rmse: 8.02798\tvalid_1's rmse: 8.01581\n",
      "[1050]\ttraining's rmse: 8.02636\tvalid_1's rmse: 8.01478\n",
      "[1100]\ttraining's rmse: 8.02468\tvalid_1's rmse: 8.01363\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1150]\ttraining's rmse: 8.02297\tvalid_1's rmse: 8.01238\n",
      "[1200]\ttraining's rmse: 8.0214\tvalid_1's rmse: 8.01166\n",
      "[1250]\ttraining's rmse: 8.02005\tvalid_1's rmse: 8.01085\n",
      "[1300]\ttraining's rmse: 8.01866\tvalid_1's rmse: 8.01014\n",
      "[1350]\ttraining's rmse: 8.01722\tvalid_1's rmse: 8.00928\n",
      "[1400]\ttraining's rmse: 8.01583\tvalid_1's rmse: 8.00844\n",
      "[1450]\ttraining's rmse: 8.01457\tvalid_1's rmse: 8.00776\n",
      "[1500]\ttraining's rmse: 8.01332\tvalid_1's rmse: 8.00716\n",
      "[1550]\ttraining's rmse: 8.01201\tvalid_1's rmse: 8.00671\n",
      "[1600]\ttraining's rmse: 8.01083\tvalid_1's rmse: 8.00621\n",
      "[1650]\ttraining's rmse: 8.00968\tvalid_1's rmse: 8.00559\n",
      "[1700]\ttraining's rmse: 8.00871\tvalid_1's rmse: 8.00512\n",
      "[1750]\ttraining's rmse: 8.00781\tvalid_1's rmse: 8.00479\n",
      "[1800]\ttraining's rmse: 8.00681\tvalid_1's rmse: 8.0044\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1850]\ttraining's rmse: 8.00585\tvalid_1's rmse: 8.00395\n",
      "[1900]\ttraining's rmse: 8.0049\tvalid_1's rmse: 8.00352\n",
      "[1950]\ttraining's rmse: 8.00389\tvalid_1's rmse: 8.00309\n",
      "[2000]\ttraining's rmse: 8.00292\tvalid_1's rmse: 8.00266\n",
      "[2050]\ttraining's rmse: 8.00199\tvalid_1's rmse: 8.00226\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2100]\ttraining's rmse: 8.00111\tvalid_1's rmse: 8.00196\n",
      "[2150]\ttraining's rmse: 8.00034\tvalid_1's rmse: 8.00172\n",
      "[2200]\ttraining's rmse: 7.9995\tvalid_1's rmse: 8.00137\n",
      "[2250]\ttraining's rmse: 7.99877\tvalid_1's rmse: 8.00118\n",
      "[2300]\ttraining's rmse: 7.99796\tvalid_1's rmse: 8.00088\n",
      "[2350]\ttraining's rmse: 7.99716\tvalid_1's rmse: 8.00062\n",
      "[2400]\ttraining's rmse: 7.99628\tvalid_1's rmse: 8.0004\n",
      "[2450]\ttraining's rmse: 7.99554\tvalid_1's rmse: 8.00021\n",
      "[2500]\ttraining's rmse: 7.99486\tvalid_1's rmse: 7.99994\n",
      "[2550]\ttraining's rmse: 7.99413\tvalid_1's rmse: 7.99982\n",
      "[2600]\ttraining's rmse: 7.9934\tvalid_1's rmse: 7.99967\n",
      "[2650]\ttraining's rmse: 7.99269\tvalid_1's rmse: 7.99963\n",
      "[2700]\ttraining's rmse: 7.99198\tvalid_1's rmse: 7.99942\n",
      "[2750]\ttraining's rmse: 7.9913\tvalid_1's rmse: 7.99929\n",
      "[2800]\ttraining's rmse: 7.99062\tvalid_1's rmse: 7.99918\n",
      "[2850]\ttraining's rmse: 7.99002\tvalid_1's rmse: 7.99906\n",
      "[2900]\ttraining's rmse: 7.98937\tvalid_1's rmse: 7.99892\n",
      "[2950]\ttraining's rmse: 7.98874\tvalid_1's rmse: 7.9988\n",
      "[3000]\ttraining's rmse: 7.98808\tvalid_1's rmse: 7.99878\n",
      "[3050]\ttraining's rmse: 7.98742\tvalid_1's rmse: 7.99863\n",
      "[3100]\ttraining's rmse: 7.98679\tvalid_1's rmse: 7.99854\n",
      "[3150]\ttraining's rmse: 7.98618\tvalid_1's rmse: 7.99849\n",
      "[3200]\ttraining's rmse: 7.98554\tvalid_1's rmse: 7.9984\n",
      "[3250]\ttraining's rmse: 7.98493\tvalid_1's rmse: 7.9983\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3300]\ttraining's rmse: 7.98441\tvalid_1's rmse: 7.99823\n",
      "[3350]\ttraining's rmse: 7.98382\tvalid_1's rmse: 7.99803\n",
      "[3400]\ttraining's rmse: 7.98325\tvalid_1's rmse: 7.99797\n",
      "[3450]\ttraining's rmse: 7.98268\tvalid_1's rmse: 7.99787\n",
      "[3500]\ttraining's rmse: 7.98211\tvalid_1's rmse: 7.99775\n",
      "[3550]\ttraining's rmse: 7.98151\tvalid_1's rmse: 7.99761\n",
      "[3600]\ttraining's rmse: 7.98095\tvalid_1's rmse: 7.99749\n",
      "[3650]\ttraining's rmse: 7.98042\tvalid_1's rmse: 7.99734\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3700]\ttraining's rmse: 7.97987\tvalid_1's rmse: 7.9973\n",
      "[3750]\ttraining's rmse: 7.97937\tvalid_1's rmse: 7.99728\n",
      "[3800]\ttraining's rmse: 7.97882\tvalid_1's rmse: 7.99721\n",
      "[3850]\ttraining's rmse: 7.97825\tvalid_1's rmse: 7.99721\n",
      "[3900]\ttraining's rmse: 7.9777\tvalid_1's rmse: 7.99717\n",
      "[3950]\ttraining's rmse: 7.97719\tvalid_1's rmse: 7.99709\n",
      "[4000]\ttraining's rmse: 7.9767\tvalid_1's rmse: 7.99698\n",
      "[4050]\ttraining's rmse: 7.97618\tvalid_1's rmse: 7.99693\n",
      "[4100]\ttraining's rmse: 7.97568\tvalid_1's rmse: 7.99694\n",
      "[4150]\ttraining's rmse: 7.97516\tvalid_1's rmse: 7.99683\n",
      "[4200]\ttraining's rmse: 7.97465\tvalid_1's rmse: 7.99686\n",
      "[4250]\ttraining's rmse: 7.97415\tvalid_1's rmse: 7.99687\n",
      "[4300]\ttraining's rmse: 7.97368\tvalid_1's rmse: 7.99677\n",
      "[4350]\ttraining's rmse: 7.97324\tvalid_1's rmse: 7.99673\n",
      "[4400]\ttraining's rmse: 7.97273\tvalid_1's rmse: 7.99668\n",
      "[4450]\ttraining's rmse: 7.97227\tvalid_1's rmse: 7.99669\n",
      "[4500]\ttraining's rmse: 7.97182\tvalid_1's rmse: 7.99664\n",
      "[4550]\ttraining's rmse: 7.97137\tvalid_1's rmse: 7.99656\n",
      "[4600]\ttraining's rmse: 7.97096\tvalid_1's rmse: 7.99655\n",
      "[4650]\ttraining's rmse: 7.97051\tvalid_1's rmse: 7.99661\n",
      "[4700]\ttraining's rmse: 7.97007\tvalid_1's rmse: 7.99661\n",
      "[4750]\ttraining's rmse: 7.96962\tvalid_1's rmse: 7.99656\n",
      "[4800]\ttraining's rmse: 7.96915\tvalid_1's rmse: 7.99649\n",
      "[4850]\ttraining's rmse: 7.96872\tvalid_1's rmse: 7.99647\n",
      "[4900]\ttraining's rmse: 7.96833\tvalid_1's rmse: 7.99642\n",
      "[4950]\ttraining's rmse: 7.96787\tvalid_1's rmse: 7.99645\n",
      "[5000]\ttraining's rmse: 7.96744\tvalid_1's rmse: 7.99649\n",
      "[5050]\ttraining's rmse: 7.96699\tvalid_1's rmse: 7.99653\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5100]\ttraining's rmse: 7.96657\tvalid_1's rmse: 7.99649\n",
      "[5150]\ttraining's rmse: 7.96617\tvalid_1's rmse: 7.9965\n",
      "Early stopping, best iteration is:\n",
      "[4969]\ttraining's rmse: 7.96771\tvalid_1's rmse: 7.99639\n",
      "Train RMSE: 7.967714736129603        Valida RMSE: 7.996387850685034\n",
      "OOF RMSE: 8.02182905271538 \n"
     ]
    }
   ],
   "source": [
    "results,models,importances,oof,feature_list = Training_Lightgbm(df_train[columns_modeling_last],params,fold_column = 'fold',target_column = 'target',cat_vars = cat_columns ,metric = 'RMSE',early_stopping = 200,max_boost_round = 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot((oof),label = 'oof')\n",
    "sns.distplot(df_train['target'],label = 'target')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin_predict\n",
      "fin_predict\n",
      "fin_predict\n",
      "fin_predict\n",
      "fin_predict\n",
      "Real:  7.969145198912088\n"
     ]
    }
   ],
   "source": [
    "probs = 0\n",
    "for i in models:\n",
    "    probs = probs + (i.predict(df_test[feature_list]))\n",
    "    \n",
    "    print('fin_predict')\n",
    "y_test_pred = probs/5.0\n",
    "print(f'Real: ',math.sqrt(mean_squared_error(y_test_pred,df_test['Target'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37320</th>\n",
       "      <td>6.686630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3913</th>\n",
       "      <td>6.379808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112180</th>\n",
       "      <td>12.148823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128820</th>\n",
       "      <td>12.207944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16037</th>\n",
       "      <td>6.379808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           target\n",
       "Index            \n",
       "37320    6.686630\n",
       "3913     6.379808\n",
       "112180  12.148823\n",
       "128820  12.207944\n",
       "16037    6.379808"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_submission['target'] = y_test_pred\n",
    "y_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enviar los resultados\n",
    "#apiquery.submit_api(y_submission,\n",
    "#       competition_name='food',\n",
    "#        subname='test_v1', # Pueden cambiar esto sin problemas, poner el nombre que quieran.\n",
    "#        holdout_key='None',\n",
    "#        update_ldb=True,\n",
    "#        username=\"Insight ML - DD\" # Poner el nombre de su equipo como un string. \n",
    "                                  # El mejor de los resultados dentro de sus envios es el que aparecera en la tabla de posiciones.\n",
    "#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
